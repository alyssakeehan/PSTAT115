---
title: "HW1"
author: "Alyssa Keehan"
date: "4/25/2021"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
library(knitr)
library(testthat)
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center')
indent1 = '    '
indent2 = paste(rep(indent1, 2), collapse='')
indent3 = paste(rep(indent1, 3), collapse='')
r = function(x, digits=2){ round(x, digits=digits) }
library(tidyverse)
library(reshape2)
```

__Note:__ If you are working with a partner, please submit only one homework per group with both names and whether you are taking the course for graduate credit or not.  Submit your Rmarkdown (.Rmd) and the compiled pdf on Gauchospace.
 
# 1. Cancer Research in Laboratory Mice

A laboratory is estimating the rate of tumorigenesis (the formation of tumors) in two strains of mice, A and B. They have tumor count data for 10 mice in strain A and 13 mice in strain B.  Type A mice have been well studied, and information from other laboratories suggests that type A mice have tumor counts that are approximately Poisson-distributed. Tumor count rates for type B mice are unknown, but type B mice are related to type A mice. Assuming a Poisson sampling distribution for each group with rates $\theta_A$ and $\theta_B$. Based on previous research you settle on the following prior distribution:

$$ \theta_A \sim \text{gamma}(120, 10),\ \theta_B\sim\text{gamma}(12, 1)$$ 

## a. Before seeing any data, which group do you expect to have a higher average incidence of cancer? Which group are you more certain about a priori? You answers should be based on the priors specified above.

Below I will calculate the variance of theta given their apriori gamma distribution.
```{r}
Avar <- 120/100
bvar <- 12
Avar
bvar
```
**Answer**
_Since the expected value of each group of mice both come out to be 12, both groups have the same average incidence of cancer. As computed above, the variance for type A is much less than the variance for type B. That being said, a smaller variance would imply more certainty about the expectation of the incidence of cancer._ 

    
## b.  After you the complete of the experiment, you  observe the following tumor counts for the two populations: 
$$y_A = (12,9,12,14,13,13,15,8,15,6)$$
$$y_B = (11,11,10,9,9,8,7,10,6,8,8,9,7)$$
    
## Compute the posterior parameters, posterior means, posterior variances and 95% quantile-based credible intervals for $\theta_A$ and $\theta_B$. Same them in the appropriate variables in the code cell below.  You do not need to show your work, but you cannot get partial credit unless you do show work.

```{r summary_stats, echo=FALSE}
yA <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB <- c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)
```
```{r}
### Prior parameters here
alpha_A = 120 
beta_A = 10 

alpha_B = 12 
beta_B = 1 
```

Using slide 35 from the One parameter models lecture, I will update the alpha values to be used for the posterior gamma distribution.

posterior alpha: sum of observation values for that specific group + prior alpha

posterior beta: number of observations for that specific group + prior beta
```{r}
### Posterior parameters here
alpha_A_posterior = sum(yA) + alpha_A 
beta_A_posterior = length(yA) + beta_A 

alpha_B_posterior = sum(yB) + alpha_B 
beta_B_posterior = length(yB) + beta_B 
```

Using slide 31 of the One Parameter Model Slides, and the parameters we computed above, I will plug them into the equations for the mean and Variance for each distribution.

posterior mean : posterior alpha / posterior beta

posterior variance = posterior alpha / posterior beta^2 
```{r}
### Posterior mean and variance for each group        
A_post_mean <- alpha_A_posterior/beta_A_posterior 
A_post_var <- alpha_A_posterior/(beta_A_posterior^2)

### Posterior quantiles for each group
B_post_mean <-  alpha_B_posterior/beta_B_posterior 
B_post_var <- alpha_B_posterior/(beta_B_posterior^2) 

print(paste0("Posterior mean of theta_A ", round(A_post_mean, 2)))
print(paste0("Posterior variance of theta_A ", round(A_post_var, 2)))
print(paste0("Posterior mean of theta_B ", round(B_post_mean, 2)))
print(paste0("Posterior variance of theta_B ", round(B_post_var, 2)))
```
I computed the below quantiles using qgamma
```{r}
# Posterior quantiles
alpha_A_quantile <- c(qgamma(0.05 / 2, alpha_A_posterior, beta_A_posterior),
                      qgamma(1 - (0.05 / 2), alpha_A_posterior, beta_A_posterior))
alpha_B_quantile <- c(qgamma(0.05 / 2, alpha_B_posterior, beta_B_posterior),
                      qgamma(1 - (0.05 / 2), alpha_B_posterior, beta_B_posterior))

print(paste0("Posterior 95% quantile for theta_A is [",
             round(alpha_A_quantile[1],2), ", ",
             round(alpha_A_quantile[2], 2), "]"))
print(paste0("Posterior 95% quantile for theta_B is [",
             round(alpha_B_quantile[1],2), ", ",
             round(alpha_B_quantile[2], 2), "]"))
```

## c. Compute and plot the posterior expectation of $\theta_B$ given $y_B$ under the prior distribution  $\text{gamma}(12\times n_0, n_0)$ for each value of $n_0 \in \{1,2,...,50\}$. As a reminder, $n_0$ can be thought of as the number of prior observations (or pseudo-counts).  

```{r}
# update the new alpha and beta values
n0 <- seq(1:50)
new_alphaB <- n0*12
new_betaB <- n0

# compute new posterior alpha and betas
alpha_B_posterior2 <- sum(yB) + new_alphaB # YOUR CODE HERE
beta_B_posterior2 <- length(yB) + new_betaB # YOUR CODE HERE

# use the equation for mean of gamma distribution
posterior_means <- alpha_B_posterior2/beta_B_posterior2 # YOUR CODE HERE

# plotting the posterior means against n0
post_means <- data.frame(n0,posterior_means)
ggplot(post_means, aes(n0,posterior_means)) +
  geom_line() +
  xlab('Number of Observations') +
  ylab('Posterior Means')
```

## d. Should knowledge about population A tell us anything about population B? Discuss whether or not it makes sense to have $p(\theta_A, \theta_B) = p(\theta_A) \times p(\theta_B)$.  

**Answer** I believe that knowledge about population A can tell us some things about population B because it is said that these groups are blood related. Since the two populations are related, we can conclude that there is a correlation between these groups.Because they is some correlation between the two popultions, this implies that they are not independent of each other. So since these two populations are not independent of each other, it does not make sense that p(theta_a,theta_b) = p(theta_a)*p(theta_b).

\vspace{.2in}

# 2. A Mixture Prior for Heart Transplant Surgeries

A hospital in the United States wants to evaluate their success rate of heart transplant surgeries.  We observe the number of deaths, $y$, in a number of heart transplant surgeries. Let $y \sim \text{Pois}(\nu\lambda)$ where $\lambda$ is the rate of deaths/patient and $\nu$ is the exposure (total number of heart transplant patients).  When measuring rare events with low rates, maximum likelihood estimation can be notoriously bad.  We'll tak a Bayesian approach.  To construct your prior distribution you talk to two experts.  The first expert thinks that $p_1(\lambda)$ with a \text{gamma}(3, 2000)$ density is a reasonable prior. The second expert thinks that $p_2(\lambda)$ with a $\text{gamma}(7, 1000)$ density is a reasonable prior distribution.  You decide that each expert is equally credible so you combine their prior distributions into a mixture prior with equal weights: $p(\lambda) = 0.5 * p_1(\lambda) + 0.5 * p_2(\lambda)$

## a. What does each expert think the mean rate is, _a priori_? Which expert is more confident about the value of $\lambda$ a priori (i.e. before seeing any data)?
```{r}
first_mean <- 3/2000
second_mean <- 7/1000
first_variance <- 3/(2000**2)
second_variance <- 7/(1000**2)
print(paste('The first expert thinks the mean rate of deaths/patient a-priori is',first_mean))
print(paste('The second expert thinks the mean rate of deaths/patient a-priori is',second_mean))
print(paste('The first expert thinks the variance of deaths/patient a-priori is',first_variance))
print(paste('The second expert thinks the variance of deaths/patient a-priori is',second_variance))
```
**Answer**_The first expert thinks the a priori rate is 0.0015 and the second expert thinks the a priori rate is 0.007.Before seeing any data, I believe that the first expert is more confident about the value of lambda a priori because their predicted distribution contians a higher beta value than than the second expert. In addition, the variance for the first expert's predicted distribution is lower than that of the second expert's predition. Because of this, I believe the first expert is more confident about the vaue of their lambda a-priori._
    
## b. Plot the mixture prior distribution.
Since the mixture distribution is as follows:
$$prior = 0.5 *\frac{2000^{3}}{\Gamma(3)}\lambda^{3-1}e^{-2000\lambda} + 0.5 *\frac{1000^{7}}{\Gamma(7)}\lambda^{7-1}e^{-1000\lambda}$$
We can input it directly into the curve function.

```{r}
curve(0.5 * 2000^3/gamma(3) * l^(3-1) * exp(-2000 * l) 
              + 0.5 * 1000^7/gamma(7) * l^(7-1) * exp(-1000 * l),
      from = 0, to = 0.03, xname = 'l', xlab = "lambda", ylab = "density")
```

## c. Suppose the hospital has $y=8$ deaths with an exposure of $\nu=1767$ surgeries performed.  Write the posterior distribution up to a proportionality constant by multiplying the likelihood and the prior density.  _Warning:_ be very careful about what constitutes a proportionality constant in this example.
Since $x = 8$ and $\nu = 1767$, the likelihood is just $$poisson(\nu\lambda) = e^{-\nu\lambda}*\frac{(\nu\lambda)^8}{8!}$$

And the prior is given by $p(\lambda) = 0.5 * p_1(\lambda) + 0.5 * p_2(\lambda)$ with $$p_1(\lambda) = 0.5 * \frac{2000^3}{\Gamma(3)}\lambda^{3-1}e^{-2000*\lambda}$$ and $$p_2(\lambda) = 0.5 * \frac{1000^7}{\Gamma(7)}\lambda^{7-1}e^{-1000*\lambda}$$

then we can multiply those two to get the posterior distribution below.
$$ posterior \propto e^{-1767*\lambda}*\frac{(1767*\lambda)^8}{8!}*(0.5 * \frac{2000^3}{\Gamma(3)}\lambda^{3-1}e^{-2000*\lambda} + 
0.5 * \frac{1000^7}{\Gamma(7)}\lambda^{7-1}e^{-1000*\lambda})$$

## d. Let $K = \int L(\lambda; y)p(\lambda) d\lambda$ be the integral of the proportional posterior.  Then the proper posterior density, i.e. a true density integrates to 1, can be expressed as $p(\lambda \mid y) = \frac{L(\lambda; y)p(\lambda)}{K}$.  Compute this posterior density and clearly express the density as a mixture of two gamma distributions. 

In order to neatly integrate this $$K = \int L(\lambda; y)p(\lambda) d\lambda$$, I split the inside equations into two parts. $$first = \int^{\infty}_{0} e^{-3767*\lambda}*\frac{1767^8}{8!}*0.5 * \frac{2000^3}{\Gamma(3)}\lambda^{10}d\lambda$$ and $$second = \int^{\infty}_{0} e^{-2767*\lambda}*\frac{1767^8}{8!}*0.5 * \frac{1000^7}{\Gamma(7)}\lambda^{14}d\lambda$$. After taking the constants out, I get
$$K = 0.5 * \frac{2000^3}{\Gamma(3)} * \frac{1767^8}{8!} \int^{\infty}_{0}\lambda^{10}e^{-3767 * \lambda}d\lambda + 0.5 * \frac{1000^7}{\Gamma(7)} * \frac{1767^8}{8!}* \int^{\infty}_{0}\lambda^{14}e^{-2767 * \lambda}d\lambda$$
Using the gamma trick, $\int^{\infty}_{0}x^{\alpha-1}e^{-\beta x}dx = \frac{\Gamma(\alpha)}{\beta^\alpha}$, I can use this trick to simplify below.
$$K=0.5 * \frac{2000^3}{\Gamma(3)} * \frac{1767^8}{8!} * \frac{\Gamma(11)}{3767^{11}} + 0.5 * \frac{1000^7}{\Gamma(7)} * \frac{1767^8}{8!} * \frac{\Gamma(15)}{2767^{15}} = 0.04133356$$
```{r}
K <- 0.5 * (2000^3/gamma(3)) * (1767^8/factorial(8)) * (gamma(11)/3767^11) +
  0.5 * (1000^7/gamma(7)) * (1767^8 / factorial(8)) * (gamma(15)/2767^15)
K
```
Computing the integral ends with a mixture density og Gamma(11,3767) and Gamma(15,2767).Simplifiying the gamma distribution variables by proportionality, I get a final posterior of
$$posterior = \frac{1767^8 * 0.5 * 2000^3}{(K)(8!) \Gamma(3)}\lambda^{10}e^{-3767 \lambda} + \frac{1767^8 * 0.5 * 1000^7}{(K)(8!) \Gamma(7)}\lambda^{14}e^{-2767 \lambda}$$

## e.  Plot the posterior distribution. Add vertical lines clearly indicating the prior means from each expert. Also add a vertical line for the maximum likelihood estimate. 

```{r}
# separate the two coeffiecients so easier computation
first_part_post <- 1767^8 * 0.5 * 2000^3/(K*factorial(8)*gamma(3))
second_part_post <- 1767^8 * 0.5 * 1000^7/(K*factorial(8)*gamma(7))

#plot the posterior
curve(first_part_post * l^10 * exp(-3767 * l) + second_part_post * l^14 * exp(-2767 * l),
      from = 0, to = 0.03, xname = "l", 
      xlab = "lambda", ylab = "density", cex = 4)

# add vertical lines
# prior means
abline(v = 3/2000, col = 'red', cex = 1) 
abline(v = 7/1000, col = 'red', cex = 1)

# mle
abline(v = 8/1767, col = 'blue', cex = 1)

# add a legend
legend(0.017,200, legend = c('mle','prior means'),
       col = c('blue','red'), lty = c(1,1), cex = 1)
```

